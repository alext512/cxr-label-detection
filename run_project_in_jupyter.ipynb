{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e70b357",
   "metadata": {},
   "source": [
    "# Run the refactored CXR MIL project in Jupyter\n",
    "\n",
    "This notebook:\n",
    "1) Unzips the project  \n",
    "2) Installs it in editable mode  \n",
    "3) Verifies PyTorch + CUDA  \n",
    "4) Runs training via the CLI module\n",
    "\n",
    "Set the `ZIP_PATH` and `DATA_ROOT` variables below to match your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd63a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 0) Set paths ---\n",
    "import os, sys, pathlib\n",
    "\n",
    "ZIP_PATH = \"/workspace/pyproject.zip\"   # <-- change if needed\n",
    "PROJECT_DIR = \"/workspace/project_flexible_preserve_aspect_ratio\"   # where to unzip\n",
    "DATA_ROOT = \"/workspace/data\"                                 # <-- change if needed\n",
    "\n",
    "\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"ZIP_PATH exists?\", os.path.exists(ZIP_PATH))\n",
    "print(\"DATA_ROOT exists?\", os.path.exists(DATA_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec9705a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Unzip project ---\n",
    "import shutil, subprocess, pathlib, os\n",
    "\n",
    "# If project already exists → skip unzip\n",
    "if os.path.exists(PROJECT_DIR) and os.listdir(PROJECT_DIR):\n",
    "    print(\"Project already exists, skipping unzip.\")\n",
    "else:\n",
    "    print(\"Extracting project to:\", PROJECT_DIR)\n",
    "    os.makedirs(PROJECT_DIR, exist_ok=True)\n",
    "\n",
    "    cmd = f'unzip -q \"{ZIP_PATH}\" -d \"{PROJECT_DIR}\"'\n",
    "    print(\"Running:\", cmd)\n",
    "    subprocess.check_call(cmd, shell=True)\n",
    "\n",
    "# Detect actual project root (some zips include a single top-level folder)\n",
    "entries = [p for p in pathlib.Path(PROJECT_DIR).iterdir()]\n",
    "if len(entries) == 1 and entries[0].is_dir():\n",
    "    PROJECT_ROOT = str(entries[0])\n",
    "else:\n",
    "    PROJECT_ROOT = PROJECT_DIR\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a4e67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2) Install project (editable) ---\n",
    "import subprocess, sys\n",
    "\n",
    "# If torch/torchvision are missing in THIS kernel env, install them first, then rerun this cell.\n",
    "cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-e\", PROJECT_ROOT]\n",
    "print(\"Running:\", \" \".join(cmd))\n",
    "subprocess.check_call(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c2d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3) Verify torch + CUDA ---\n",
    "import torch\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"gpu:\", torch.cuda.get_device_name(0))\n",
    "    print(\"capability:\", torch.cuda.get_device_capability(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76922bc-d82d-497e-b126-e6441a9f26ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464c0616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4) Quick sanity: import the package ---\n",
    "import cxr_mil\n",
    "print(\"cxr_mil imported from:\", cxr_mil.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c05978f",
   "metadata": {},
   "source": [
    "## Optional: download dataset from Kaggle\n",
    "\n",
    "This matches the original notebook’s Kaggle download step.\n",
    "\n",
    "**Dataset slug:** `alexandrostsikalas/grand-xray-slam-resized-512`\n",
    "\n",
    "### What you need\n",
    "- A Kaggle API token (`access_token`).\n",
    "  - In Kaggle: *Account → API → Create New Token*.\n",
    "  - Upload `access_token` to the notebook environment.\n",
    "\n",
    "If your data already exists under `DATA_ROOT`, you can skip this section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faa5981-4e46-4784-86fc-223affd3c0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add access_token in the workspace folder\n",
    "!mkdir -p /root/.kaggle\n",
    "!mv /workspace/access_token /root/.kaggle/access_token\n",
    "!chmod 600 /root/.kaggle/access_token\n",
    "!pip install kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d81339-3817-4bbe-8553-0fce28c81915",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "\n",
    "DATA_DIR=/workspace/data\n",
    "LOG_FILE=\"$DATA_DIR/download_progress.log\"\n",
    "mkdir -p \"$DATA_DIR\"\n",
    ": > \"$LOG_FILE\"\n",
    "\n",
    "log(){ echo \"$@\" | tee -a \"$LOG_FILE\"; }\n",
    "\n",
    "log \"==== DOWNLOAD SESSION START $(date) ====\"\n",
    "log \"DATA_DIR=$DATA_DIR\"\n",
    "log \"LOG_FILE=$LOG_FILE\"\n",
    "\n",
    "# -----------------------------\n",
    "# Monitor download size: append to LOG_FILE every +5GB downloaded (log-only)\n",
    "# Sums sizes of ALL .zip files in DATA_DIR (handles multi-zip downloads).\n",
    "# -----------------------------\n",
    "(\n",
    "  THRESHOLD=$((5*1024*1024*1024))  # 5GB in bytes\n",
    "  NEXT=$THRESHOLD\n",
    "\n",
    "  while true; do\n",
    "    total=0\n",
    "    shopt -s nullglob\n",
    "    for f in \"$DATA_DIR\"/*.zip; do\n",
    "      sz=$(stat -c%s \"$f\" 2>/dev/null || echo 0)\n",
    "      total=$((total + sz))\n",
    "    done\n",
    "    shopt -u nullglob\n",
    "\n",
    "    while [ \"$total\" -ge \"$NEXT\" ]; do\n",
    "      gb=$((NEXT/1024/1024/1024))\n",
    "      echo \"Download progress: ${gb}GB reached\" >> \"$LOG_FILE\"\n",
    "      NEXT=$((NEXT + THRESHOLD))\n",
    "    done\n",
    "\n",
    "    sleep 5\n",
    "  done\n",
    ") &\n",
    "MON_PID=$!\n",
    "\n",
    "cleanup_monitor() {\n",
    "  kill \"$MON_PID\" 2>/dev/null || true\n",
    "  wait \"$MON_PID\" 2>/dev/null || true\n",
    "}\n",
    "trap cleanup_monitor EXIT\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Download (choose ONE). Progress bar stays in notebook.\n",
    "# -----------------------------\n",
    "log \"Downloading...\"\n",
    "\n",
    "# OPTION A: Competition\n",
    "# kaggle competitions download -c grand-xray-slam-division-a -p \"$DATA_DIR\"\n",
    "\n",
    "# OPTION B: Dataset\n",
    "kaggle datasets download -d alexandrostsikalas/grand-xray-slam-resized-512 -p \"$DATA_DIR\"\n",
    "\n",
    "cleanup_monitor\n",
    "trap - EXIT\n",
    "log \"Download finished.\"\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Unzip quietly + remove ZIP(s)\n",
    "# -----------------------------\n",
    "log \"Looking for ZIP files in $DATA_DIR ...\"\n",
    "shopt -s nullglob\n",
    "zips=( \"$DATA_DIR\"/*.zip )\n",
    "shopt -u nullglob\n",
    "\n",
    "if [ ${#zips[@]} -eq 0 ]; then\n",
    "  log \"ERROR: No ZIP files found in $DATA_DIR\"\n",
    "  exit 1\n",
    "fi\n",
    "\n",
    "log \"Unzipping ${#zips[@]} file(s)...\"\n",
    "for z in \"${zips[@]}\"; do\n",
    "  log \"Extracting $(basename \"$z\") ...\"\n",
    "  unzip -oq \"$z\" -d \"$DATA_DIR\" >> \"$LOG_FILE\" 2>&1\n",
    "  rm -f \"$z\"\n",
    "  log \"Removed $(basename \"$z\")\"\n",
    "done\n",
    "\n",
    "log \"DONE $(date)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c853aa2d-b9e9-4224-92a6-609d9f04be36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Jupyter terminal\n",
    "!rm -r ~/.local/share/Trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a93e893-dacf-41cf-a8d3-078cdd3b03a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, subprocess, shlex, textwrap\n",
    "\n",
    "WEIGHTS_DIR = \"/workspace/weights\"\n",
    "os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# URLs (official sources)\n",
    "# -----------------------------\n",
    "CHEX_URL = \"https://huggingface.co/torchxrayvision/densenet121-res224-chex/resolve/main/model.pt\"\n",
    "ALL_DENSENET_URL = \"https://github.com/mlmed/torchxrayvision/releases/download/v1/nih-pc-chex-mimic_ch-google-openi-kaggle-densenet121-d121-tw-lr001-rot45-tr15-sc15-seed0-best.pt\"\n",
    "ALL_RESNET50_512_URL = \"https://github.com/mlmed/torchxrayvision/releases/download/v1/pc-nih-rsna-siim-vin-resnet50-test512-e400-state.pt\"\n",
    "\n",
    "# -----------------------------\n",
    "# Local paths\n",
    "# -----------------------------\n",
    "SRC_CHEX   = f\"{WEIGHTS_DIR}/densenet121-res224-chex__model.pt\"\n",
    "SRC_ALL_DN = f\"{WEIGHTS_DIR}/densenet121-res224-all.pt\"\n",
    "SRC_ALL_R50= f\"{WEIGHTS_DIR}/resnet50-res512-all.pt\"\n",
    "\n",
    "OUT_CHEX   = f\"{WEIGHTS_DIR}/densenet121-res224-chex__state_dict.pth\"\n",
    "OUT_ALL_DN = f\"{WEIGHTS_DIR}/densenet121-res224-all__state_dict.pth\"\n",
    "OUT_ALL_R50= f\"{WEIGHTS_DIR}/resnet50-res512-all__state_dict.pth\"\n",
    "\n",
    "# Minimum “this is not an HTML error page” sizes (bytes). Very conservative.\n",
    "MIN_SIZES = {\n",
    "    SRC_CHEX:   5_000_000,   # ~28MB expected\n",
    "    SRC_ALL_DN: 5_000_000,   # ~28MB expected\n",
    "    SRC_ALL_R50:10_000_000,  # ~90MB expected\n",
    "}\n",
    "\n",
    "def have(cmd):\n",
    "    return subprocess.call([\"bash\",\"-lc\", f\"command -v {shlex.quote(cmd)} >/dev/null 2>&1\"]) == 0\n",
    "\n",
    "def run_bash(cmd):\n",
    "    print(\"\\n>>> bash -lc\", cmd)\n",
    "    subprocess.check_call([\"bash\", \"-lc\", cmd])\n",
    "\n",
    "def ok_file(path, min_size=1):\n",
    "    return os.path.exists(path) and os.path.getsize(path) >= min_size\n",
    "\n",
    "def download(url, dst, min_size=1):\n",
    "    # If already good, keep it\n",
    "    if ok_file(dst, min_size):\n",
    "        print(f\"OK: exists {dst} ({os.path.getsize(dst)/1e6:.1f} MB)\")\n",
    "        return\n",
    "\n",
    "    # Remove bad/partial file\n",
    "    if os.path.exists(dst):\n",
    "        print(f\"Removing incomplete file: {dst}\")\n",
    "        os.remove(dst)\n",
    "\n",
    "    tmp = dst + \".part\"\n",
    "    if os.path.exists(tmp):\n",
    "        os.remove(tmp)\n",
    "\n",
    "    if have(\"curl\"):\n",
    "        run_bash(f\"curl -fL --retry 3 --retry-delay 2 -o {shlex.quote(tmp)} {shlex.quote(url)}\")\n",
    "    else:\n",
    "        run_bash(f\"wget -q --show-progress --progress=bar:force:noscroll -O {shlex.quote(tmp)} -L {shlex.quote(url)}\")\n",
    "\n",
    "    if not ok_file(tmp, min_size):\n",
    "        raise RuntimeError(f\"Download failed or file too small: {url} -> {tmp}\")\n",
    "\n",
    "    os.replace(tmp, dst)\n",
    "    print(f\"Saved: {dst} ({os.path.getsize(dst)/1e6:.1f} MB)\")\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Download (idempotent)\n",
    "# -----------------------------\n",
    "download(CHEX_URL, SRC_CHEX, MIN_SIZES[SRC_CHEX])\n",
    "download(ALL_DENSENET_URL, SRC_ALL_DN, MIN_SIZES[SRC_ALL_DN])\n",
    "download(ALL_RESNET50_512_URL, SRC_ALL_R50, MIN_SIZES[SRC_ALL_R50])\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Install minimal deps only if needed (idempotent)\n",
    "# -----------------------------\n",
    "# We need torchxrayvision ONLY if a file is pickled and requires allowlisted unpickle.\n",
    "# Installing it is cheap (no-deps), and safe to re-run.\n",
    "run_bash(f\"{shlex.quote(sys.executable)} -m pip install -U --no-deps torchxrayvision\")\n",
    "\n",
    "# Ensure small deps torchxrayvision imports (also safe to re-run)\n",
    "run_bash(f\"{shlex.quote(sys.executable)} -m pip install -U imageio scikit-image pandas tqdm requests pillow\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Convert to pure state_dict (idempotent)\n",
    "# -----------------------------\n",
    "def convert_if_needed(src, dst):\n",
    "    if ok_file(dst, 1_000_000):  # converted state_dicts should not be tiny\n",
    "        print(f\"OK: converted exists {dst} ({os.path.getsize(dst)/1e6:.1f} MB)\")\n",
    "        return\n",
    "\n",
    "    py = textwrap.dedent(r\"\"\"\n",
    "    import torch, sys\n",
    "\n",
    "    SRC = sys.argv[1]\n",
    "    DST = sys.argv[2]\n",
    "\n",
    "    def is_state_dict(d):\n",
    "        if not isinstance(d, dict) or len(d) == 0:\n",
    "            return False\n",
    "        v = next(iter(d.values()))\n",
    "        return torch.is_tensor(v)\n",
    "\n",
    "    def extract_state_dict(obj):\n",
    "        if is_state_dict(obj):\n",
    "            return obj\n",
    "        if isinstance(obj, dict):\n",
    "            for k in [\"state_dict\",\"model_state_dict\",\"model\",\"net\",\"backbone\",\"encoder\"]:\n",
    "                if k in obj:\n",
    "                    cand = obj[k]\n",
    "                    if hasattr(cand, \"state_dict\"):\n",
    "                        return cand.state_dict()\n",
    "                    if is_state_dict(cand):\n",
    "                        return cand\n",
    "            raise ValueError(\"Can't find state_dict in checkpoint. Keys: \" + str(list(obj.keys())[:30]))\n",
    "        if hasattr(obj, \"state_dict\"):\n",
    "            return obj.state_dict()\n",
    "        raise TypeError(\"Unsupported loaded object type: \" + str(type(obj)))\n",
    "\n",
    "    def allowlisted_unpickle_load(path):\n",
    "        import torch.serialization\n",
    "        import torchxrayvision as xrv\n",
    "\n",
    "        allow = []\n",
    "        for name in [\"DenseNet\", \"_DenseLayer\", \"_DenseBlock\", \"_Transition\",\n",
    "                     \"ResNet\", \"BasicBlock\", \"Bottleneck\"]:\n",
    "            if hasattr(xrv.models, name):\n",
    "                allow.append(getattr(xrv.models, name))\n",
    "        if not allow:\n",
    "            raise RuntimeError(\"Could not build allowlist from torchxrayvision.models\")\n",
    "\n",
    "        with torch.serialization.safe_globals(allow):\n",
    "            return torch.load(path, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "    print(\"\\n=== Converting ===\")\n",
    "    print(\"SRC:\", SRC)\n",
    "    print(\"DST:\", DST)\n",
    "\n",
    "    # Try safe weights-only load first\n",
    "    try:\n",
    "        obj = torch.load(SRC, map_location=\"cpu\", weights_only=True)\n",
    "        print(\"Loaded with weights_only=True (safe).\")\n",
    "    except Exception as e:\n",
    "        msg = str(e)\n",
    "        if (\"Weights only load failed\" in msg) or (\"Unsupported global\" in msg) or (\"safe_globals\" in msg):\n",
    "            print(\"Pickle detected. Using allowlisted unpickle (trusted source)...\")\n",
    "            obj = allowlisted_unpickle_load(SRC)\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    sd = extract_state_dict(obj)\n",
    "    torch.save(sd, DST)\n",
    "\n",
    "    print(\"Saved:\", DST)\n",
    "    print(\"Num keys:\", len(sd))\n",
    "    print(\"First keys:\", list(sd.keys())[:8])\n",
    "    \"\"\")\n",
    "\n",
    "    subprocess.check_call([sys.executable, \"-c\", py, src, dst])\n",
    "    if not ok_file(dst, 1_000_000):\n",
    "        raise RuntimeError(f\"Conversion produced suspiciously small file: {dst}\")\n",
    "\n",
    "convert_if_needed(SRC_CHEX, OUT_CHEX)\n",
    "convert_if_needed(SRC_ALL_DN, OUT_ALL_DN)\n",
    "convert_if_needed(SRC_ALL_R50, OUT_ALL_R50)\n",
    "\n",
    "print(\"\\n✅ Final files (use these in YAML checkpoint_path):\")\n",
    "print(\"  \", OUT_CHEX)\n",
    "print(\"  \", OUT_ALL_DN)\n",
    "print(\"  \", OUT_ALL_R50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce072e5a-bf5d-4bcc-9732-f36bef56bcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"imageio\"])\n",
    "print(\"✔ imageio installed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2e0f1f",
   "metadata": {},
   "source": [
    "## Run training\n",
    "\n",
    "This calls the CLI using the **same Python** as the notebook kernel (`sys.executable`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6400ae-f4ff-4b92-a398-c4a155a9d6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess, shlex, os\n",
    "\n",
    "CONFIGS_DIR = os.path.join(PROJECT_ROOT, \"configs\")\n",
    "\n",
    "yamls = [\n",
    "\n",
    "    \"exp12_imagenet_densenet121_512_resize_preserve_ratio.yaml\",\n",
    "\n",
    "]\n",
    "\n",
    "for y in yamls:\n",
    "    CONFIG_PATH = os.path.join(CONFIGS_DIR, y)\n",
    "    if not os.path.exists(CONFIG_PATH):\n",
    "        raise FileNotFoundError(f\"Missing YAML: {CONFIG_PATH}\")\n",
    "\n",
    "    cmd = [\n",
    "        sys.executable, \"-m\", \"cxr_mil.train_cv\",\n",
    "        \"--root\", DATA_ROOT,\n",
    "        \"--config\", CONFIG_PATH,\n",
    "    ]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"Running:\", y)\n",
    "    print(\"Command:\\n\", \" \".join(shlex.quote(c) for c in cmd))\n",
    "\n",
    "    rc = subprocess.call(cmd)\n",
    "    print(\"Process finished with return code:\", rc)\n",
    "\n",
    "    if rc != 0:\n",
    "        raise RuntimeError(f\"Training failed for {y} (return code {rc}).\")\n",
    "\n",
    "print(\"\\n✅ All experiments finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e51922a",
   "metadata": {},
   "source": [
    "## Optional: check GPU status\n",
    "\n",
    "Run this in a separate cell to see GPU utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7144c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "try:\n",
    "    subprocess.check_call(\"nvidia-smi\", shell=True)\n",
    "except Exception as e:\n",
    "    print(\"nvidia-smi not available:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
