# Equivalent-style legacy recipe with cleaner run hygiene.
experiment:
  run_name: exp_legacy_weighted_bce_plateau
  seed: 42
  output_root: runs

data:
  root: /path/to/data
  csv_name: train1.csv
  img_subdir: train1

model:
  backbone:
    name: densenet121
    pretrained: true
    kwargs: {}
  head:
    name: view_aware_gated_attn
    kwargs: {hidden_dim: 512, attn_dim: 256, dropout: 0.30, attn_dropout: 0.10}

cv: {n_splits: 5, select_fold: null}

train:
  total_epochs: 20
  frozen_epochs: 2
  head_lr: 0.0003
  backbone_lr: 0.00001
  weight_decay: 0.0001
  accum_steps: 4
  study_batch_size: 48
  max_grad_norm: null
  num_workers: 8

loss: {name: weighted_bce, pos_weight_clip: 50.0}
scheduler: {name: reduce_on_plateau, factor: 0.5, patience: 2}
ema: {enabled: false, decay: 0.999}

augmentations:
  train:
    - {name: resize, size: 512}
    - {name: hflip, p: 0.5}
    - {name: rotation, degrees: 7}
    - {name: color_jitter, brightness: 0.08, contrast: 0.08, saturation: 0.05, hue: 0.02}
    - {name: to_tensor}
    - {name: normalize, mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225]}
  val:
    - {name: resize, size: 512}
    - {name: to_tensor}
    - {name: normalize, mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225]}
